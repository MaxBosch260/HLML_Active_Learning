{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMR8V2ZVSGXm"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-activeml\n",
        "!pip install skorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM2Nni_nFTCJ"
      },
      "source": [
        "# Research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4gcoLVmFTCJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mlp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import warnings\n",
        "\n",
        "from copy import deepcopy\n",
        "from skactiveml.classifier import SklearnClassifier\n",
        "from skactiveml.pool import UncertaintySampling, QueryByCommittee, RandomSampling\n",
        "from skactiveml.utils import call_func\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from skorch import NeuralNetClassifier\n",
        "from torch import nn\n",
        "\n",
        "mlp.rcParams[\"figure.facecolor\"] = \"white\"\n",
        "\n",
        "MISSING_LABEL = -1\n",
        "### REMOVE SEEDS TO REDUCE COMPUTATION TIME ###\n",
        "random_states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
        "FONTSIZE = 20\n",
        "\n",
        "batch_sizes = [1, 2, 4, 5, 10, 20]\n",
        "total_samples = 100\n",
        "\n",
        "cycles_per_batch_size = []\n",
        "for batch_size in batch_sizes:\n",
        "  cycles_per_batch_size.append(int(total_samples/batch_size))\n",
        "\n",
        "\n",
        "# Define base module.\n",
        "class ClassifierModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClassifierModule, self).__init__()\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.dense_layer = nn.Linear(288, len(classes))\n",
        "        self.outpout = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.conv_layer(X)\n",
        "        X = X.reshape(X.size(0), -1)\n",
        "        X= self.dense_layer(X)\n",
        "        X = self.outpout(X)\n",
        "        return X\n",
        "\n",
        "results = []\n",
        "\n",
        "for state in random_states:\n",
        "  print(f'Random state: {state}')\n",
        "  torch.manual_seed(state)\n",
        "  torch.cuda.manual_seed(state)\n",
        "\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "  # Load digit data set.\n",
        "  X, y_true = load_digits(return_X_y=True)\n",
        "\n",
        "  # Standardize data.\n",
        "  X = StandardScaler().fit_transform(X)\n",
        "\n",
        "  # Reshape samples to n_samples x n_channels x width x height to fit skorch\n",
        "  # requirements.\n",
        "  X = X.reshape((len(X), 1, 8, 8))\n",
        "\n",
        "  # Set data types according to skorch requirements.\n",
        "  X, y_true = X.astype(np.float32), y_true.astype(np.int64)\n",
        "\n",
        "  # Identify list of possible classes.\n",
        "  classes = np.unique(y_true)\n",
        "\n",
        "  # Make a 66-34 train-test split.\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "      X, y_true, train_size=0.66, random_state=state\n",
        "  )\n",
        "\n",
        "  # Create list of three base CNNs.\n",
        "  learning_rates = [1.e-3, 1.e-2, 1.e-1]\n",
        "  estimators = []\n",
        "  for i, learning_rate in enumerate(learning_rates):\n",
        "      net = NeuralNetClassifier(\n",
        "          ClassifierModule,\n",
        "          max_epochs=100,\n",
        "          lr=learning_rate,\n",
        "          verbose=0,\n",
        "          train_split=False,\n",
        "      )\n",
        "      net.initialize()\n",
        "      estimators.append((f'clf {i}',\n",
        "                        SklearnClassifier(\n",
        "                            estimator=net, missing_label=MISSING_LABEL,\n",
        "                            random_state=i, classes=classes)\n",
        "                        )\n",
        "                        )\n",
        "\n",
        "  # Creat voting ensemble out of given ensemble list.\n",
        "  ensemble_init = SklearnClassifier(\n",
        "      estimator=VotingClassifier(estimators=estimators, voting='soft'),\n",
        "      missing_label=MISSING_LABEL, random_state=state, classes=classes\n",
        "  )\n",
        "\n",
        "  qs_dict = {\n",
        "      'random sampling': RandomSampling(random_state=state, missing_label=MISSING_LABEL),\n",
        "      'uncertainty sampling (least confident)': UncertaintySampling(method='least_confident', random_state=state, missing_label=MISSING_LABEL),\n",
        "      'uncertainty sampling (entropy)': UncertaintySampling(method='entropy', random_state=state, missing_label=MISSING_LABEL),\n",
        "      'uncertainty sampling (margin)': UncertaintySampling(method='margin_sampling', random_state=state, missing_label=MISSING_LABEL),\n",
        "      'query-by-committee': QueryByCommittee(random_state=state, missing_label=MISSING_LABEL),\n",
        "  }\n",
        "\n",
        "  batch_acc_dictionaries = []\n",
        "  for i, batch_size in enumerate(batch_sizes):\n",
        "    acc_dict = {key: np.zeros(cycles_per_batch_size[i]+1) for key in qs_dict}\n",
        "\n",
        "    print(f'Batch size used: {batch_size}')\n",
        "    # Perform active learning with each query strategy.\n",
        "    for qs_name, qs in qs_dict.items():\n",
        "\n",
        "      print(f'Execute active learning using {qs_name}')\n",
        "\n",
        "      # Copy initial ensemble model.\n",
        "      ensemble = deepcopy(ensemble_init)\n",
        "\n",
        "      # Create array of missing labels as initial labels.\n",
        "      y = np.full_like(y_train, fill_value=MISSING_LABEL, dtype=np.int64)\n",
        "\n",
        "      # Execute active learning cycle.\n",
        "      for c in range(cycles_per_batch_size[i]):\n",
        "          # Fit and evaluate ensemble.\n",
        "          acc = ensemble.fit(X_train, y).score(X_test, y_test)\n",
        "          acc_dict[qs_name][c] = acc\n",
        "\n",
        "          # Select and update training data.\n",
        "          query_idx = call_func(\n",
        "              qs.query, X=X_train, y=y, clf=ensemble, fit_clf=False, ensemble=ensemble,\n",
        "              fit_ensemble=False, batch_size=batch_size\n",
        "          )\n",
        "          y[query_idx] = y_train[query_idx]\n",
        "\n",
        "      # Fit and evaluate ensemble.\n",
        "      ensemble.fit(X_train, y)\n",
        "      acc_dict[qs_name][cycles_per_batch_size[i]] = ensemble.score(X_test, y_test)\n",
        "\n",
        "    batch_acc_dictionaries.append(acc_dict)\n",
        "\n",
        "  results.append(batch_acc_dictionaries)\n",
        "\n",
        "\n",
        "#print(results)\n",
        "#print(len(results))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIxT7mgmFTCK"
      },
      "outputs": [],
      "source": [
        "FONTSIZE = 30\n",
        "\n",
        "averaged_batch_accuracies = []\n",
        "for cycles in cycles_per_batch_size:\n",
        "  sum = np.zeros((len(qs_dict), cycles+1))\n",
        "  averaged_batch_accuracies.append(sum)\n",
        "\n",
        "# sum results of seeds\n",
        "for state_results in results:\n",
        "  for i, batch_acc_dictionary in enumerate(state_results):\n",
        "    for j, accuracies in enumerate(batch_acc_dictionary.values()):\n",
        "      averaged_batch_accuracies[i][j] += accuracies\n",
        "\n",
        "# average accuracies over seeds\n",
        "for i, accuracies in enumerate(averaged_batch_accuracies):\n",
        "  averaged_batch_accuracies[i] = accuracies/len(random_states)\n",
        "\n",
        "for i, batch_accuracies in enumerate(averaged_batch_accuracies):\n",
        "  cycles = np.arange(0, total_samples+1, batch_sizes[i], dtype=int)\n",
        "  plt.figure(figsize=(16, 9))\n",
        "  for index, qs_name in enumerate(qs_dict.keys()):\n",
        "    plt.plot(cycles, batch_accuracies[index], label=f'{qs_name}: AULC={round(batch_accuracies[index].mean(), 2)}')\n",
        "  plt.xticks(fontsize=FONTSIZE)\n",
        "  plt.yticks(fontsize=FONTSIZE)\n",
        "  plt.title(f'Batch size: {batch_sizes[i]}', fontsize=FONTSIZE)\n",
        "  plt.xlabel('Number of samples', fontsize=FONTSIZE)\n",
        "  plt.ylabel('Test accuracy', fontsize=FONTSIZE)\n",
        "  plt.legend(loc='lower right', fontsize='xx-large')\n",
        "  plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}